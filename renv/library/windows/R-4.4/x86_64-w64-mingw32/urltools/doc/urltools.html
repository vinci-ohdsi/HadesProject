<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="generator" content="litedown 0.7">
<title></title>
<style type="text/css">
body {
  font-family: sans-serif;
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 1.5;
  print-color-adjust: exact;
  -webkit-print-color-adjust: exact;
}
body, .abstract, code, .footnotes, footer, #refs, .caption { font-size: .9em; }
li li { font-size: .95em; }
ul:has(li > input[type="checkbox"]) { list-style: none; padding-left: 1em; }
*, :before, :after { box-sizing: border-box; }
a { color: steelblue; }
pre, img { max-width: 100%; }
pre { white-space: pre-wrap; word-break: break-word; }
pre code { display: block; padding: 1em; overflow-x: auto; }
code { font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace; }
:not(pre, th) > code, code[class], div > .caption { background: #f8f8f8; }
pre > code:is(:not([class]), .language-plain, .language-none, .plain), .box, .figure, .table { background: inherit; border: 1px solid #eee; }
pre > code {
  &.message { border-color: #9eeaf9; }
  &.warning { background: #fff3cd; border-color: #fff3cd; }
  &.error { background: #f8d7da; border-color: #f8d7da; }
}
.fenced-chunk { border-left: 1px solid #666; }
.code-fence {
  opacity: .4;
  border: 1px dashed #666;
  border-left: 2px solid;
  &:hover { opacity: inherit; }
}
.box, .figure, .table, table { margin: 1em auto; }
div > .caption { padding: 1px 1em; }
.figure { p:has(img, svg), pre:has(svg) { text-align: center; } }
.flex-col { display: flex; justify-content: space-between; }
table {
  &:only-child:not(.table > *) { margin: auto; }
  th, td { padding: 5px; font-variant-numeric: tabular-nums; }
  thead, tfoot, tr:nth-child(even) { background: whitesmoke; }
  thead th { border-bottom: 1px solid #ddd; }
  &:not(.datatable-table) {
    border-top: 1px solid #666;
    border-bottom: 1px solid #666;
  }
}
blockquote {
  color: #666;
  margin: 0;
  padding: 1px 1em;
  border-left: .5em solid #eee;
}
hr, .footnotes::before { border: 1px dashed #ddd; }
.frontmatter { text-align: center; }
#TOC {
  a { text-decoration: none; }
  ul { list-style: none; padding-left: 1em; }
  & > ul { padding: 0; }
  ul ul { border-left: 1px solid lightsteelblue; }
}
.body h2 { border-bottom: 1px solid #666; }
.body .appendix, .appendix ~ h2 { border-bottom-style: dashed; }
.main-number::after { content: "."; }
span[class^="ref-number-"] { font-weight: bold; }
.ref-number-fig::after, .ref-number-tab::after { content: ":"; }
.cross-ref-chp::before { content: "Chapter "; }
.cross-ref-sec::before { content: "Section "; }
.cross-ref-fig::before, .ref-number-fig::before { content: "Figure "; }
.cross-ref-tab::before, .ref-number-tab::before { content: "Table "; }
.cross-ref-eqn::before, .MathJax_ref:has(mjx-mtext > mjx-c + mjx-c)::before { content: "Equation "; }
.abstract, #refs {
  &::before { display: block; margin: 1em auto; font-weight: bold; }
}
.abstract::before { content: "Abstract"; text-align: center; }
#refs::before { content: "Bibliography"; font-size: 1.5em; }
.ref-paren-open::before { content: "("; }
.ref-paren-close::after { content: ")"; }
.ref-semicolon::after { content: "; "; }
.ref-and::after { content: " and "; }
.ref-et-al::after { content: " et al."; font-style: italic; }
.footnote-ref a {
  &::before { content: "["; }
  &::after { content: "]"; }
}
section.footnotes {
  margin-top: 2em;
  &::before { content: ""; display: block; max-width: 20em; }
}
.fade {
  background: repeating-linear-gradient(135deg, white, white 30px, #ddd 32px, #ddd 32px);
  opacity: 0.6;
}

@media print {
  body { max-width: 100%; }
  tr, img { break-inside: avoid; }
}
@media only screen and (min-width: 992px) {
  body:not(.pagesjs) pre:has(.line-numbers):not(:hover) { white-space: pre; }
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@xiee/utils@1.14.14/css/prism-xcode.min.css">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js" defer></script>
</head>
<body>
<div class="frontmatter">
</div>
<div class="body">
<!--
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{urltools}
-->
<h2 id="sec:elegant-url-handling-with-urltools">Elegant URL handling with urltools</h2>
<p>URLs are treated, by base R, as nothing more than components of a data retrieval process: they exist
to create connections to retrieve datasets. This is an essential feature for the language to have,
but it also means that URL handlers are designed for situations where URLs <em>get</em> you to the data -
not situations where URLs <em>are</em> the data.</p>
<p>There is no support for encoding or decoding URLs en-masse, and no support for parsing and
interpreting them. <code>urltools</code> provides this support!</p>
<h3 id="sec:url-encoding-and-decoding">URL encoding and decoding</h3>
<p>Base R provides two functions - <code>URLdecode</code> and <code>URLencode</code> - for taking percentage-encoded
URLs and turning them into regular strings, or vice versa. As discussed, these are primarily designed to
enable connections, and so they have several inherent limitations, including a lack of vectorisation, that
make them unsuitable for large datasets.</p>
<p>Not only are they not vectorised, they also have several particularly idiosyncratic bugs and limitations:
<code>URLdecode</code>, for example, breaks if the decoded value is out of range:</p>
<pre><code class="language-r">URLdecode(&quot;test%gIL&quot;)
Error in rawToChar(out) : embedded nul in string: '\0L'
In addition: Warning message:
In URLdecode(&quot;%gIL&quot;) : out-of-range values treated as 0 in coercion to raw
</code></pre>
<p>URLencode, on the other hand, encodes slashes on its most strict setting - without
paying attention to where those slashes <em>are</em>: if we attempt to URLencode an entire URL, we get:</p>
<pre><code class="language-r">URLencode(&quot;https://en.wikipedia.org/wiki/Article&quot;, reserved = TRUE)
[1] &quot;https%3a%2f%2fen.wikipedia.org%2fwiki%2fArticle&quot;
</code></pre>
<p>That’s a completely unusable URL (or ewRL, if you will).</p>
<p>urltools replaces both functions with <code>url_decode</code> and <code>url_encode</code> respectively:</p>
<pre><code class="language-r">library(urltools)
url_decode(&quot;test%gIL&quot;)
[1] &quot;test&quot;
url_encode(&quot;https://en.wikipedia.org/wiki/Article&quot;)
[1] &quot;https://en.wikipedia.org%2fwiki%2fArticle&quot;
</code></pre>
<p>As you can see, <code>url_decode</code> simply excludes out-of-range characters from consideration, while <code>url_encode</code> detects characters that make up part of the URLs scheme, and leaves them unencoded. Both are extremely fast; with <code>urltools</code>, you can
decode a vector of 1,000,000 URLs in 0.9 seconds.</p>
<p>Alongside these, we have functions for encoding and decoding the ‘punycode’ format of URLs - ones that are designed to be internationalised and have unicode characters in them. These also take one argument, a vector of URLs, and can be found at <code>puny_encode</code> and <code>puny_decode</code> respectively.</p>
<h3 id="sec:url-parsing">URL parsing</h3>
<p>Once you’ve got your nicely decoded (or encoded) URLs, it’s time to do something with them - and, most of the time,
you won’t actually care about most of the URL. You’ll want to look at the scheme, or the domain, or the path,
but not the entire thing as one string.</p>
<p>The solution is <code>url_parse</code>, which takes a URL and breaks it out into its <a href="http://www.ietf.org/rfc/rfc3986.txt">RfC 3986</a> components: scheme, domain, port, path, query string and fragment identifier. This is,
again, fully vectorised, and can happily be run over hundreds of thousands of URLs, rapidly processing them. The
results are provided as a data.frame, since most people use data.frames to store data.</p>
<pre><code class="language-r">&gt; parsed_address &lt;- url_parse(&quot;https://en.wikipedia.org/wiki/Article&quot;)
&gt; str(parsed_address)
'data.frame':	1 obs. of  6 variables:
 $ scheme   : chr &quot;https&quot;
 $ domain   : chr &quot;en.wikipedia.org&quot;
 $ port     : chr NA
 $ path     : chr &quot;wiki/Article&quot;
 $ parameter: chr NA
 $ fragment : chr NA                         
</code></pre>
<p>We can also perform the opposite of this operation with <code>url_compose</code>:</p>
<pre><code class="language-r">&gt; url_compose(parsed_address)
[1] &quot;https://en.wikipedia.org/wiki/article&quot;
</code></pre>
<h3 id="sec:getting-setting-url-components">Getting/setting URL components</h3>
<p>With the inclusion of a URL parser, we suddenly have the opportunity for lubridate-style component getting
and setting. Syntax is identical to that of <code>lubridate</code>, but uses URL components as function names.</p>
<pre><code class="language-r">url &lt;- &quot;https://en.wikipedia.org/wiki/Article&quot;
scheme(url)
&quot;https&quot;
scheme(url) &lt;- &quot;ftp&quot;
url
&quot;ftp://en.wikipedia.org/wiki/Article&quot;
</code></pre>
<p>Fields that can be extracted or set are <code>scheme</code>, <code>domain</code>, <code>port</code>, <code>path</code>,
<code>parameters</code> and <code>fragment</code>.</p>
<h3 id="sec:suffix-and-tld-extraction">Suffix and TLD extraction</h3>
<p>Once we’ve extracted a domain from a URL with <code>domain</code> or <code>url_parse</code>, we can identify which bit is the domain name, and which
bit is the suffix:</p>
<pre><code class="language-r">&gt; url &lt;- &quot;https://en.wikipedia.org/wiki/Article&quot;
&gt; domain_name &lt;- domain(url)
&gt; domain_name
[1] &quot;en.wikipedia.org&quot;
&gt; str(suffix_extract(domain_name))
'data.frame':	1 obs. of  4 variables:
 $ host     : chr &quot;en.wikipedia.org&quot;
 $ subdomain: chr &quot;en&quot;
 $ domain   : chr &quot;wikipedia&quot;
 $ suffix      : chr &quot;org&quot;
</code></pre>
<p>This relies on an internal database of public suffixes, accessible at <code>suffix_dataset</code> - we recognise, though,
that this dataset may get a bit out of date, so you can also pass the results of the <code>suffix_refresh</code> function,
which retrieves an updated dataset, to <code>suffix_extract</code>:</p>
<pre><code class="language-r">domain_name &lt;- domain(&quot;https://en.wikipedia.org/wiki/Article&quot;)
updated_suffixes &lt;- suffix_refresh()
suffix_extract(domain_name, updated_suffixes)
</code></pre>
<p>We can do the same thing with top-level domains, with precisely the same setup, except the functions and datasets are <code>tld_refresh</code>, <code>tld_extract</code> and <code>tld_dataset</code>.</p>
<p>In the other direction we have <code>host_extract</code>, which retrieves, well, the host! If the URL has subdomains, it’ll be the
lowest-level subdomain. If it doesn’t, it’ll be the actual domain name, without the suffixes:</p>
<pre><code class="language-r">domain_name &lt;- domain(&quot;https://en.wikipedia.org/wiki/Article&quot;)
host_extract(domain_name)
</code></pre>
<h3 id="sec:query-manipulation">Query manipulation</h3>
<p>Once a URL is parsed, it’s sometimes useful to get the value associated with a particular query parameter. As
an example, take the URL <code>http://en.wikipedia.org/wiki/api.php?action=parse&amp;pageid=1023&amp;export=json</code>. What
pageID is being used? What is the export format? We can find out with <code>param_get</code>.</p>
<pre><code class="language-r">&gt; str(param_get(urls = &quot;http://en.wikipedia.org/wiki/api.php?action=parse&amp;pageid=1023&amp;export=json&quot;,
                     parameter_names = c(&quot;pageid&quot;,&quot;export&quot;)))
'data.frame':	1 obs. of  2 variables:
 $ pageid: chr &quot;1023&quot;
 $ export: chr &quot;json&quot;
</code></pre>
<p>This isn’t the only function for query manipulation; we can also dynamically modify the values a particular parameter
might have, or strip them out entirely.</p>
<p>To modify the values, we use <code>param_set</code>:</p>
<pre><code class="language-r">url &lt;- &quot;http://en.wikipedia.org/wiki/api.php?action=parse&amp;pageid=1023&amp;export=json&quot;
url &lt;- param_set(url, key = &quot;pageid&quot;, value = &quot;12&quot;)
url
# [1] &quot;http://en.wikipedia.org/wiki/api.php?action=parse&amp;pageid=12&amp;export=json&quot;
</code></pre>
<p>As you can see this works pretty well; it even works in situations where the URL doesn’t <em>have</em> a query yet:</p>
<pre><code class="language-r">url &lt;- &quot;http://en.wikipedia.org/wiki/api.php&quot;
url &lt;- param_set(url, key = &quot;pageid&quot;, value = &quot;12&quot;)
url
# [1] &quot;http://en.wikipedia.org/wiki/api.php?pageid=12&quot;
</code></pre>
<p>On the other hand we might have a parameter we just don’t want any more - that can be handled with <code>param_remove</code>, which can
take multiple parameters as well as multiple URLs:</p>
<pre><code class="language-r">url &lt;- &quot;http://en.wikipedia.org/wiki/api.php?action=parse&amp;pageid=1023&amp;export=json&quot;
url &lt;- param_remove(url, keys = c(&quot;action&quot;,&quot;export&quot;))
url
# [1] &quot;http://en.wikipedia.org/wiki/api.php?pageid=1023&quot;
</code></pre>
<h3 id="sec:other-url-handlers">Other URL handlers</h3>
<p>If you have ideas for other URL handlers that would make your data processing easier, the best approach
is to either <a href="https://github.com/Ironholds/urltools/issues">request it</a> or <a href="https://github.com/Ironholds/urltools/pulls">add it</a>!</p>
</div>
</body>
</html>
